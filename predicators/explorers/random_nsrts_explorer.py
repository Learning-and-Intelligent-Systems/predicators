"""An explorer that takes random NSRTs."""

from typing import Dict, List, Set

from gym.spaces import Box

from predicators import utils
from predicators.explorers.base_explorer import BaseExplorer
from predicators.structs import NSRT, Action, DummyOption, \
    ExplorationStrategy, ParameterizedOption, Predicate, State, Task, Type, \
    _GroundNSRT, _Option


class RandomNSRTsExplorer(BaseExplorer):
    """RandomNSRTsExplorer implementation.

    Similar to RandomOptionsExplorer in that it chooses
    uniformly at random out of whichever ground nsrts are
    applicable in the current state, but different in that
    the continuous parameter of the parameterized option is
    generated by the nsrt's sampler rather than by sampling
    uniformly at random from the option's parameter space.

    This explorer is intended to be used when learning options
    via reinforcement learning with oracle samplers. In this
    setting, planning (e.g. using BilevelPlanningExplorer) is
    not feasible because refinement is slow or fails when the
    learned options aren't good enough yet.

    Note that the sampler requires the current low-level state
    as input. An alternative approach would generate a plan of
    options by planning towards the goal with the ground nsrts
    and avoid refinement by (1) inferring the terminal low-level
    state of each option (the initial low-level state of the
    subsequent option) from the option's sample, which is
    proposing a low-level subgoal, and (2) setting the simulator
    state directly to that state. Because applying dimensionality
    reduction techniques on the sample makes this approach infeasible,
    and this is something we may want to do, we avoid this approach
    for now.
    """

    def __init__(self, predicates: Set[Predicate],
                 options: Set[ParameterizedOption], types: Set[Type],
                 action_space: Box, train_tasks: List[Task],
                 nsrts: Set[NSRT]) -> None:

        super().__init__(predicates, options, types, action_space, train_tasks)
        self._nsrts = nsrts

        # Sequence of options generated during exploration.
        self._option_seq: Dict[int, List[_Option]] = {}

    @classmethod
    def get_name(cls) -> str:
        return "random_nsrts"

    def get_option_sequence(self,
                            train_task_idx: int) -> List[_Option]:
        """Get the sequence of options produced by the exploration strategy for
        a particular task."""
        return self._option_seq[train_task_idx]

    def get_exploration_strategy(self, train_task_idx: int,
                                 timeout: int) -> ExplorationStrategy:
        # Ensure random over successive calls.
        cur_option = DummyOption
        task = self._train_tasks[train_task_idx]
        self._option_seq[train_task_idx] = []

        def fallback_policy(state: State) -> Action:
            del state  # unused
            raise utils.RequestActPolicyFailure(
                "No applicable nsrt in this state!")

        def policy(state: State) -> Action:
            nonlocal cur_option

            if cur_option is DummyOption or cur_option.terminal(state):
                # Create all applicable ground NSRTs.
                ground_nsrts: List[_GroundNSRT] = []
                for nsrt in sorted(self._nsrts):
                    ground_nsrts.extend(
                        utils.all_ground_nsrts(nsrt, list(state)))

                # Sample an applicable NSRT.
                ground_nsrt = utils.sample_applicable_ground_nsrt(
                    state, ground_nsrts, self._predicates, self._rng)
                if ground_nsrt is None:
                    return fallback_policy(state)
                assert all(a.holds for a in ground_nsrt.preconditions)

                # Sample an option.
                option = ground_nsrt.sample_option(state,
                                                   goal=task.goal,
                                                   rng=self._rng)
                cur_option = option

                # Store option (so that we can attribute experience
                # in the environment to each option when learning
                # options via RL).
                self._option_seq[train_task_idx].append(option)

            act = cur_option.policy(state)
            return act

        # Never terminate (until the interaction budget is exceeded).
        termination_function = lambda _: False
        return policy, termination_function
