"""The core algorithm for learning a collection of NSRT data structures."""

from __future__ import annotations

import logging
from typing import FrozenSet, Iterator, List, Set, Tuple

from predicators.src import utils
from predicators.src.nsrt_learning.option_learning import create_option_learner
from predicators.src.nsrt_learning.sampler_learning import learn_samplers
from predicators.src.nsrt_learning.segmentation import segment_trajectory
from predicators.src.nsrt_learning.strips_learning import \
    learn_strips_operators
from predicators.src.planning import task_plan_grounding
from predicators.src.predicate_search_score_functions import \
    _PredictionErrorScoreFunction
from predicators.src.settings import CFG
from predicators.src.structs import NSRT, GroundAtom, GroundAtomTrajectory, \
    LowLevelTrajectory, OptionSpec, PartialNSRTAndDatastore, Predicate, \
    Segment, STRIPSOperator, Task, _GroundNSRT


def learn_nsrts_from_data(trajectories: List[LowLevelTrajectory],
                          train_tasks: List[Task], predicates: Set[Predicate],
                          sampler_learner: str) -> Set[NSRT]:
    """Learn NSRTs from the given dataset of low-level transitions, using the
    given set of predicates."""
    logging.info(f"\nLearning NSRTs on {len(trajectories)} trajectories...")

    # STEP 1: Apply predicates to data, producing a dataset of abstract states.
    ground_atom_dataset = utils.create_ground_atom_dataset(
        trajectories, predicates)

    # STEP 2: Segment each trajectory in the dataset based on changes in
    #         either predicates or options. If we are doing option learning,
    #         then the data will not contain options, so this segmenting
    #         procedure only uses the predicates.
    segmented_trajs = [
        segment_trajectory(traj) for traj in ground_atom_dataset
    ]

    # Each entry in ground_atom_dataset is a tuple of (low-level trajectory,
    # low-level ground atoms sequence). We remove the latter, because
    # it's prone to causing bugs -- we should rarely care about the
    # low-level ground atoms sequence after segmentation.
    low_level_trajs = [ll_traj for ll_traj, _ in ground_atom_dataset]
    del ground_atom_dataset

    # If performing goal-conditioned sampler learning, we need to attach the
    # goals to the segments.
    if CFG.sampler_learning_use_goals:
        for segment_traj, ll_traj in zip(segmented_trajs, low_level_trajs):
            # If the trajectory is not a demonstration, it does not have a
            # known goal (e.g., it was generated by random replay data), so we
            # won't attach a goal to the segment.
            if ll_traj.is_demo:
                goal = train_tasks[ll_traj.train_task_idx].goal
                for segment in segment_traj:
                    segment.set_goal(goal)

    segments = [seg for segs in segmented_trajs for seg in segs]

    # STEP 3: Cluster the data by effects, jointly producing one STRIPSOperator,
    #         Datastore, and OptionSpec per cluster. These items are then
    #         used to initialize PartialNSRTAndDatastore objects (PNADs).
    #         Note: The OptionSpecs here are extracted directly from the data.
    #         If we are doing option learning, then the data will not contain
    #         options, and so the option_spec fields are just the specs of a
    #         DummyOption. We need a default dummy because future steps require
    #         the option_spec field to be populated, even if just with a dummy.
    pnads = learn_strips_operators(segments,
                                   verbose=(CFG.option_learner != "no_learning"
                                            or CFG.learn_side_predicates))

    # STEP 4: Learn side predicates for the operators and update PNADs. These
    #         are predicates whose truth value becomes unknown (for *any*
    #         grounding not explicitly in effects) upon operator application.
    if CFG.learn_side_predicates:
        assert CFG.option_learner == "no_learning", \
            "Can't learn options and side predicates together."
        pnads = _learn_pnad_side_predicates(pnads, trajectories,
                                            ground_atom_dataset, train_tasks,
                                            predicates, segmented_trajs)

    # STEP 5: Learn options (option_learning.py) and update PNADs.
    _learn_pnad_options(pnads)  # in-place update

    # STEP 6: Learn samplers (sampler_learning.py) and update PNADs.
    _learn_pnad_samplers(pnads, sampler_learner)  # in-place update

    # STEP 7: Log and return the NSRTs.
    nsrts = [pnad.make_nsrt() for pnad in pnads]
    logging.info("\nLearned NSRTs:")
    for nsrt in nsrts:
        logging.info(nsrt)
    logging.info("")
    return set(nsrts)


def _learn_pnad_side_predicates(
        pnads: List[PartialNSRTAndDatastore],
        ll_trajs: List[LowLevelTrajectory],
        ground_atom_dataset: List[GroundAtomTrajectory],
        train_tasks: List[Task], predicates: Set[Predicate],
        segmented_trajs: List[List[Segment]]) -> List[PartialNSRTAndDatastore]:

    def _check_goal(s: Tuple[PartialNSRTAndDatastore, ...]) -> bool:
        del s  # unused
        # There are no goal states for this search; run until exhausted.
        return False

    def _get_successors(
        s: Tuple[PartialNSRTAndDatastore, ...],
    ) -> Iterator[Tuple[None, Tuple[PartialNSRTAndDatastore, ...], float]]:
        # For each PNAD/operator...
        for i in range(len(s)):
            pnad = s[i]
            _, option_vars = pnad.option_spec
            # ...consider changing each of its effects to a side predicate.
            for effect in pnad.op.add_effects:
                if len(pnad.op.add_effects) > 1:
                    # We don't want sidelining to result in a no-op
                    new_pnad = PartialNSRTAndDatastore(
                        pnad.op.effect_to_side_predicate(
                            effect, option_vars, "add"), pnad.datastore,
                        pnad.option_spec)
                    sprime = list(s)
                    sprime[i] = new_pnad
                    yield (None, tuple(sprime), 1.0)

            # ...consider removing it.
            sprime = list(s)
            del sprime[i]
            yield (None, tuple(sprime), 1.0)

    if CFG.sidelining_approach == "naive":
        score_func = _PredictionErrorScoreFunction(predicates, [], {},
                                                   train_tasks)

        def _evaluate(s: Tuple[PartialNSRTAndDatastore, ...]) -> float:
            # Score function for search. Lower is better.
            strips_ops = [pnad.op for pnad in s]
            option_specs = [pnad.option_spec for pnad in s]
            score = score_func.evaluate_with_operators(frozenset(), ll_trajs,
                                                       segmented_trajs,
                                                       strips_ops,
                                                       option_specs)
            return score

    elif CFG.sidelining_approach == "preserve_skeletons":

        def _evaluate(s: Tuple[PartialNSRTAndDatastore, ...]) -> float:
            # Score function for search. Lower is better.
            strips_ops = [pnad.op for pnad in s]
            option_specs = [pnad.option_spec for pnad in s]
            preserves_harmlessness = check_harmlessness(
                predicates, train_tasks, ground_atom_dataset, strips_ops,
                option_specs)
            # NOTE: Arbitrary large number bigger than total number of
            # operators
            score = 10000
            if preserves_harmlessness:
                score = 2 * len(strips_ops)
                for op in strips_ops:
                    score -= len(op.side_predicates)
            return score

    else:
        raise ValueError(
            f"sidelining_approach {CFG.sidelining_approach} not implemented")

    # Run the search, starting from original PNADs.
    path, _, _ = utils.run_hill_climbing(tuple(pnads), _check_goal,
                                         _get_successors, _evaluate)
    # The last state in the search holds the final PNADs.
    pnads = list(path[-1])
    # Recompute the datastores in the PNADs. We need to do this
    # because now that we have side predicates, each transition may be
    # assigned to *multiple* datastores.
    _recompute_datastores_from_segments(segmented_trajs, pnads)
    return pnads


def _recompute_datastores_from_segments(
        segmented_trajs: List[List[Segment]],
        pnads: List[PartialNSRTAndDatastore]) -> None:
    for pnad in pnads:
        pnad.datastore = []  # reset all PNAD datastores
    for seg_traj in segmented_trajs:
        objects = set(seg_traj[0].states[0])
        for segment in seg_traj:
            assert segment.has_option()
            segment_option = segment.get_option()
            segment_param_option = segment_option.parent
            segment_option_objs = tuple(segment_option.objects)
            # Get ground operators given these objects and option objs.
            for pnad in pnads:
                param_opt, opt_vars = pnad.option_spec
                if param_opt != segment_param_option:
                    continue
                isub = dict(zip(opt_vars, segment_option_objs))
                # Consider adding this segment to each datastore.
                for ground_op in utils.all_ground_operators_given_partial(
                        pnad.op, objects, isub):
                    # Check if preconditions hold.
                    if not ground_op.preconditions.issubset(
                            segment.init_atoms):
                        continue
                    # Check if effects match. Note that we're using the side
                    # predicates semantics here!
                    atoms = utils.apply_operator(ground_op, segment.init_atoms)
                    if not atoms.issubset(segment.final_atoms):
                        continue
                    # Skip over segments that have multiple possible bindings.
                    if len(set(ground_op.objects)) != len(ground_op.objects):
                        continue
                    # This segment belongs in this datastore, so add it.
                    sub = dict(zip(pnad.op.parameters, ground_op.objects))
                    pnad.add_to_datastore((segment, sub),
                                          check_effect_equality=False)


def _learn_pnad_options(pnads: List[PartialNSRTAndDatastore]) -> None:
    logging.info("\nDoing option learning...")
    option_learner = create_option_learner()
    strips_ops = []
    datastores = []
    for pnad in pnads:
        strips_ops.append(pnad.op)
        datastores.append(pnad.datastore)
    option_specs = option_learner.learn_option_specs(strips_ops, datastores)
    assert len(option_specs) == len(pnads)
    # Replace the option_specs in the PNADs.
    for pnad, option_spec in zip(pnads, option_specs):
        pnad.option_spec = option_spec
    # Seed the new parameterized option parameter spaces.
    for parameterized_option, _ in option_specs:
        parameterized_option.params_space.seed(CFG.seed)
    # Update the segments to include which option is being executed.
    for datastore, spec in zip(datastores, option_specs):
        for (segment, _) in datastore:
            # Modifies segment in-place.
            option_learner.update_segment_from_option_spec(segment, spec)
    logging.info("\nLearned operators with option specs:")
    for pnad in pnads:
        logging.info(pnad)


def _learn_pnad_samplers(pnads: List[PartialNSRTAndDatastore],
                         sampler_learner: str) -> None:
    logging.info("\nDoing sampler learning...")
    strips_ops = []
    datastores = []
    option_specs = []
    for pnad in pnads:
        strips_ops.append(pnad.op)
        datastores.append(pnad.datastore)
        option_specs.append(pnad.option_spec)
    samplers = learn_samplers(strips_ops, datastores, option_specs,
                              sampler_learner)
    assert len(samplers) == len(strips_ops)
    # Replace the samplers in the PNADs.
    for pnad, sampler in zip(pnads, samplers):
        pnad.sampler = sampler


def check_harmlessness(init_preds: Set[Predicate], train_tasks: List[Task],
                       pruned_atom_data: List[GroundAtomTrajectory],
                       strips_ops: List[STRIPSOperator],
                       option_specs: List[OptionSpec]) -> bool:
    """Function to check whether a given set of operators and predicates
    preserves harmlessness over demonstrations on some number of training
    tasks."""

    for ll_traj, hl_traj in pruned_atom_data:
        if not ll_traj.is_demo:
            continue
        traj_goal = train_tasks[ll_traj.train_task_idx].goal
        plan_preserved = check_single_plan_preservation(
            ll_traj, hl_traj, traj_goal, init_preds, strips_ops, option_specs)
        if not plan_preserved:
            return False

    return True


def check_single_plan_preservation(ll_traj: LowLevelTrajectory,
                                   hl_traj: List[Set[GroundAtom]],
                                   traj_goal: Set[GroundAtom],
                                   init_preds: Set[Predicate],
                                   strips_ops: List[STRIPSOperator],
                                   option_specs: List[OptionSpec]) -> bool:
    """Function to check whether a given set of operators and predicates
    preserves a single training trajectory."""
    init_atoms = utils.abstract(ll_traj.states[0], init_preds)
    objects = set(ll_traj.states[0])
    ground_nsrts, _ = task_plan_grounding(init_atoms, objects, strips_ops,
                                          option_specs)
    heuristic = utils.create_task_planning_heuristic(
        CFG.sesame_task_planning_heuristic, init_atoms, traj_goal,
        ground_nsrts, init_preds, objects)

    def _check_goal(state: Tuple[FrozenSet[GroundAtom], int]) -> bool:
        return traj_goal.issubset(state[0])

    def _get_successor_with_correct_option(
        searchnode_state: Tuple[FrozenSet[GroundAtom], int]
    ) -> Iterator[Tuple[_GroundNSRT, Tuple[FrozenSet[GroundAtom], int],
                        float]]:
        state = searchnode_state[0]
        idx_into_traj = searchnode_state[1]

        if idx_into_traj <= len(ll_traj.actions) - 1:
            assert ll_traj.actions[idx_into_traj].has_option()
            gt_option = ll_traj.actions[idx_into_traj].get_option()
            expected_next_hl_state = hl_traj[idx_into_traj + 1]

            for applicable_nsrt in utils.get_applicable_operators(
                    ground_nsrts, state):
                # NOTE: we check that the ParameterizedOptions are equal before
                # attempting to ground because otherwise, we might
                # get a parameter mismatch and trigger an AssertionError
                # during grounding.

                if applicable_nsrt.option == gt_option.parent:
                    if applicable_nsrt.option_objs == gt_option.objects:
                        next_hl_state = utils.apply_operator(
                            applicable_nsrt, set(state))
                        # Here, we check whether all atoms that differ
                        # between next_hl_state and state are part of
                        # the operator's side predicates. If so, this nsrt
                        # can be applied from this state!
                        exp_state_matches = next_hl_state.issubset(
                            expected_next_hl_state)

                        if exp_state_matches:
                            # The returned cost is uniform because we don't
                            # actually care about finding the shortest path;
                            # just one that matches!
                            yield (applicable_nsrt, (frozenset(next_hl_state),
                                                     idx_into_traj + 1), 1.0)

    init_atoms_frozen = frozenset(init_atoms)
    init_searchnode_state = (init_atoms_frozen, 0)
    state_seq, _ = utils.run_gbfs(
        init_searchnode_state, _check_goal, _get_successor_with_correct_option,
        lambda searchnode_state: heuristic(searchnode_state[0]))

    return _check_goal(state_seq[-1])
