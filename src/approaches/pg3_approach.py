"""Policy-guided planning for generalized policy generation (PG3).

Example command line:
    python src/main.py --approach pg3 --seed 0 \
        --env pddl_easy_delivery_procedural_tasks \
        --strips_learner oracle
"""
from __future__ import annotations

import abc
import functools
import logging
from typing import FrozenSet, Iterator, List, Optional, Sequence, Set, Tuple

import dill as pkl

from predicators.src import utils
from predicators.src.approaches import ApproachFailure
from predicators.src.approaches.nsrt_metacontroller_approach import \
    NSRTMetacontrollerApproach
from predicators.src.settings import CFG
from predicators.src.structs import NSRT, AbstractTask, Box, Dataset, \
    GroundAtom, GroundAtomTrajectory, LDLRule, LiftedAtom, \
    LiftedDecisionList, LowLevelTrajectory, ParameterizedOption, Predicate, \
    State, Task, Type, Variable, _GroundNSRT


class PG3Approach(NSRTMetacontrollerApproach):
    """Policy-guided planning for generalized policy generation (PG3)."""

    def __init__(self, initial_predicates: Set[Predicate],
                 initial_options: Set[ParameterizedOption], types: Set[Type],
                 action_space: Box, train_tasks: List[Task]) -> None:
        super().__init__(initial_predicates, initial_options, types,
                         action_space, train_tasks)
        self._current_ldl = LiftedDecisionList([])

    @classmethod
    def get_name(cls) -> str:
        return "pg3"

    def _predict(self, state: State, atoms: Set[GroundAtom],
                 goal: Set[GroundAtom]) -> _GroundNSRT:
        del state  # unused
        ground_nsrt = utils.query_ldl(self._current_ldl, atoms, goal)
        if ground_nsrt is None:
            raise ApproachFailure("PG3 policy was not applicable!")
        return ground_nsrt

    def _learn_ldl(self, trajectories: Sequence[LowLevelTrajectory]) -> None:
        """Learn a lifted decision list policy."""

        # Create the PG3 search operators for search in LDL space.
        search_operators = self._create_search_operators()

        # The heuristic is what distinguishes PG3 from baseline approaches.
        heuristic = self._create_heuristic(trajectories)

        # An "action" here is a search operator and an integer representing the
        # count of successors generated by that operator.
        def get_successors(
            ldl: LiftedDecisionList
        ) -> Iterator[Tuple[Tuple[_PG3SearchOperator, int], LiftedDecisionList,
                            float]]:
            for op in search_operators:
                for i, child in enumerate(op.get_successors(ldl)):
                    yield (op, i), child, 1  # cost always 1

        if CFG.pg3_search_method == "gbfs":
            path, _ = utils.run_gbfs(
                initial_state=self._current_ldl,
                check_goal=lambda _: False,  # Terminate only after max expansions
                get_successors=get_successors,
                heuristic=heuristic,
                max_expansions=CFG.pg3_gbfs_max_expansions,
                lazy_expansion=True)

        elif CFG.pg3_search_method == "hill_climbing":
            path, _,  _ = utils.run_hill_climbing(
                initial_state=self._current_ldl,
                check_goal=lambda _: False,  # Terminate when no improvement
                get_successors=get_successors,
                heuristic=heuristic,
                enforced_depth=CFG.pg3_hc_enforced_depth)

        else:
            raise NotImplementedError("Unrecognized pg3_search_method "
                                      f"{CFG.pg3_search_method}.")

        # Save the best seen policy.
        self._current_ldl = path[-1]
        logging.info(f"Keeping best policy:\n{self._current_ldl}")
        save_path = utils.get_approach_save_path_str()
        with open(f"{save_path}_None.ldl", "wb") as f:
            pkl.dump(self._current_ldl, f)

    def learn_from_offline_dataset(self, dataset: Dataset) -> None:
        # First, learn NSRTs.
        self._learn_nsrts(dataset.trajectories, online_learning_cycle=None)
        # Now, learn the LDL policy.
        self._learn_ldl(dataset.trajectories)

    def load(self, online_learning_cycle: Optional[int]) -> None:
        load_path = utils.get_approach_load_path_str()
        with open(f"{load_path}_{online_learning_cycle}.ldl", "rb") as f:
            self._current_ldl = pkl.load(f)

    def _create_search_operators(self) -> List[_PG3SearchOperator]:
        search_operator_classes = [
            _AddRulePG3SearchOperator,
            _AddConditionPG3SearchOperator,
        ]
        types = self._types
        preds = self._get_current_predicates()
        nsrts = self._get_current_nsrts()
        return [cls(types, preds, nsrts) for cls in search_operator_classes]

    def _create_heuristic(
            self, trajectories: Sequence[LowLevelTrajectory]) -> _PG3Heuristic:
        preds = self._get_current_predicates()
        abstract_train_tasks = [
            utils.create_abstract_task(task, preds)
            for task in self._train_tasks
        ]
        ground_atom_trajs = utils.create_ground_atom_dataset(
            trajectories, preds)

        if CFG.pg3_heuristic == "policy_evaluation":
            return _PolicyEvaluationPG3Heuristic(abstract_train_tasks,
                                                 ground_atom_trajs)

        if CFG.pg3_heuristic == "plan_comparison":
            return _PlanComparisonPG3Heuristic(abstract_train_tasks,
                                               ground_atom_trajs)

        raise NotImplementedError("Unrecognized pg3_heuristic: "
                                  f"{CFG.pg3_heuristic}.")


############################## Search Operators ###############################


class _PG3SearchOperator(abc.ABC):
    """Given an LDL policy, generate zero or more successor LDL policies."""

    def __init__(self, types: Set[Type], predicates: Set[Predicate],
                 nsrts: Set[NSRT]) -> None:
        self._types = types
        self._predicates = predicates
        self._nsrts = nsrts

    @abc.abstractmethod
    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        """Generate zero or more successor LDL policies."""
        raise NotImplementedError("Override me!")


class _AddRulePG3SearchOperator(_PG3SearchOperator):
    """An operator that adds new rules to an existing LDL policy."""

    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        for idx in range(len(ldl.rules) + 1):
            for rule in self._get_candidate_rules():
                new_rules = list(ldl.rules)
                new_rules.insert(idx, rule)
                yield LiftedDecisionList(new_rules)

    @functools.lru_cache(maxsize=None)
    def _get_candidate_rules(self) -> List[LDLRule]:
        return [self._nsrt_to_rule(nsrt) for nsrt in sorted(self._nsrts)]

    @staticmethod
    def _nsrt_to_rule(nsrt: NSRT) -> LDLRule:
        """Initialize an LDLRule from an NSRT."""
        return LDLRule(
            name=nsrt.name,
            parameters=list(nsrt.parameters),
            pos_state_preconditions=set(nsrt.preconditions),
            neg_state_preconditions=set(),
            goal_preconditions=set(),
            nsrt=nsrt,
        )


class _AddConditionPG3SearchOperator(_PG3SearchOperator):
    """An operator that adds new preconditions to existing LDL rules."""

    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        for rule_idx, rule in enumerate(ldl.rules):
            rule_vars = frozenset(rule.parameters)
            for condition in self._get_candidate_conditions(rule_vars):
                # Consider adding new condition to positive preconditions,
                # negative preconditions, or goal preconditions.
                for destination in ["pos", "neg", "goal"]:
                    new_pos = set(rule.pos_state_preconditions)
                    new_neg = set(rule.neg_state_preconditions)
                    new_goal = set(rule.goal_preconditions)
                    if destination == "pos":
                        dest_set = new_pos
                    elif destination == "neg":
                        dest_set = new_neg
                    else:
                        assert destination == "goal"
                        dest_set = new_goal
                    # If the condition already exists, skip.
                    if condition in dest_set:
                        continue
                    # Special case: if the condition already exists in the
                    # positive preconditions, don't add to the negative
                    # preconditions, and vice versa.
                    if destination in ("pos", "neg") and condition in \
                        new_pos | new_neg:
                        continue
                    dest_set.add(condition)
                    parameters = sorted({
                        v
                        for c in new_pos | new_neg | new_goal
                        for v in c.variables
                    } | set(rule.nsrt.parameters))
                    # Create the new rule.
                    new_rule = LDLRule(
                        name=rule.name,
                        parameters=parameters,
                        pos_state_preconditions=new_pos,
                        neg_state_preconditions=new_neg,
                        goal_preconditions=new_goal,
                        nsrt=rule.nsrt,
                    )
                    # Create the new LDL.
                    new_rules = list(ldl.rules)
                    new_rules[rule_idx] = new_rule
                    yield LiftedDecisionList(new_rules)

    @functools.lru_cache(maxsize=None)
    def _get_candidate_conditions(
            self, variables: FrozenSet[Variable]) -> List[LiftedAtom]:
        conditions = []
        for pred in sorted(self._predicates):
            # Create fresh variables for the predicate to complement the
            # variables that already exist in the rule.
            new_vars = utils.create_new_variables(pred.types, variables)
            for condition in utils.get_all_lifted_atoms_for_predicate(
                    pred, variables | frozenset(new_vars)):
                conditions.append(condition)
        return conditions


################################ Heuristics ###################################


class _PG3Heuristic(abc.ABC):
    """Given an LDL policy, produce a score, with lower better."""

    def __init__(
            self, abstract_train_tasks: Sequence[AbstractTask],
            ground_atom_trajectories: Sequence[GroundAtomTrajectory]) -> None:
        self._abstract_train_tasks = abstract_train_tasks
        self._ground_atom_trajectories = ground_atom_trajectories

    @abc.abstractmethod
    def __call__(self, ldl: LiftedDecisionList) -> float:
        """Produce a score, with lower better."""
        raise NotImplementedError("Override me!")


class _PolicyEvaluationPG3Heuristic(_PG3Heuristic):
    """Score a policy based on the number of train tasks it solves at the
    abstract level."""

    def __call__(self, ldl: LiftedDecisionList) -> float:
        unsolved_tasks = 0
        for abstract_task in self._abstract_train_tasks:
            if not self._ldl_solves_abstract_task(ldl, abstract_task):
                unsolved_tasks += 1
        logging.debug(f"Scoring:\n{ldl}\nScore: {unsolved_tasks}")
        return unsolved_tasks

    @staticmethod
    def _ldl_solves_abstract_task(ldl: LiftedDecisionList,
                                  abstract_task: AbstractTask) -> bool:
        atoms = abstract_task.init
        goal = abstract_task.goal
        for _ in range(CFG.horizon):
            if abstract_task.goal_holds(atoms):
                return True
            ground_nsrt = utils.query_ldl(ldl, atoms, goal)
            if ground_nsrt is None:
                return False
            atoms = utils.apply_operator(ground_nsrt, atoms)
        return abstract_task.goal_holds(atoms)


class _PlanComparisonPG3Heuristic(_PG3Heuristic):
    """Score a policy based on agreement with demonstration data."""

    def __call__(self, ldl: LiftedDecisionList) -> float:
        missed_steps = 0
        for ground_atom_traj in self._ground_atom_trajectories:
            traj, atom_seq = ground_atom_traj
            assert traj.is_demo
            goal = self._abstract_train_tasks[traj.train_task_idx].goal
            assert goal.issubset(atom_seq[-1])
            missed_steps += self._count_missed_steps(ldl, atom_seq, goal)
        logging.debug(f"Scoring:\n{ldl}\nScore: {missed_steps}")
        return missed_steps

    @staticmethod
    def _count_missed_steps(ldl: LiftedDecisionList,
                            atom_seq: Sequence[Set[GroundAtom]],
                            goal: Set[GroundAtom]) -> int:
        missed_steps = 0
        for t in range(len(atom_seq) - 1):
            ground_nsrt = utils.query_ldl(ldl, atom_seq[t], goal)
            if ground_nsrt is None:
                missed_steps += 1
            else:
                predicted_atoms = utils.apply_operator(ground_nsrt,
                                                       atom_seq[t])
                if predicted_atoms != atom_seq[t + 1]:
                    missed_steps += 1
        return missed_steps
