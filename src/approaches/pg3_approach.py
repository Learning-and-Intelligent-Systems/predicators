"""Policy-guided planning for generalized policy generation (PG3).

Example command line:
    python src/main.py --approach pg3 --seed 0 \
        --env pddl_easy_delivery_procedural_tasks \
        --strips_learner oracle
"""
from __future__ import annotations

import abc
import logging
from typing import Iterator, List, Optional, Sequence, Set, Tuple

import dill as pkl

from predicators.src import utils
from predicators.src.approaches import ApproachFailure
from predicators.src.approaches.nsrt_metacontroller_approach import \
    NSRTMetacontrollerApproach
from predicators.src.settings import CFG
from predicators.src.structs import NSRT, AbstractTask, Box, Dataset, \
    GroundAtom, LDLRule, LiftedAtom, LiftedDecisionList, LowLevelTrajectory, \
    ParameterizedOption, Predicate, State, Task, Type, _GroundNSRT


class PG3Approach(NSRTMetacontrollerApproach):
    """Policy-guided planning for generalized policy generation (PG3)."""

    def __init__(self, initial_predicates: Set[Predicate],
                 initial_options: Set[ParameterizedOption], types: Set[Type],
                 action_space: Box, train_tasks: List[Task]) -> None:
        super().__init__(initial_predicates, initial_options, types,
                         action_space, train_tasks)
        self._current_ldl = LiftedDecisionList([])

    @classmethod
    def get_name(cls) -> str:
        return "pg3"

    def _predict(self, state: State, atoms: Set[GroundAtom],
                 goal: Set[GroundAtom]) -> _GroundNSRT:
        del state  # unused
        ground_nsrt = utils.query_ldl(self._current_ldl, atoms, goal)
        if ground_nsrt is None:
            raise ApproachFailure("PG3 policy was not applicable!")
        return ground_nsrt

    def _learn_ldl(self) -> None:
        """Learn a lifted decision list policy."""

        # Create the PG3 search operators for search in LDL space.
        search_operators = self._create_search_operators()

        # The heuristic is what distinguishes PG3 from baseline approaches.
        heuristic = self._create_heuristic()

        # An "action" here is a search operator and an integer representing the
        # count of successors generated by that operator.
        def get_successors(
            ldl: LiftedDecisionList
        ) -> Iterator[Tuple[Tuple[_PG3SearchOperator, int], LiftedDecisionList,
                            float]]:
            for op in search_operators:
                for i, child in enumerate(op.get_successors(ldl)):
                    yield (op, i), child, 1  # cost always 1

        path, _ = utils.run_gbfs(
            initial_state=self._current_ldl,
            check_goal=lambda _: False,  # Terminate only after max expansions
            get_successors=get_successors,
            heuristic=heuristic,
            max_expansions=CFG.pg3_max_expansions)

        # Save the best seen policy.
        self._current_ldl = path[-1]
        logging.info(f"Keeping best policy:\n{self._current_ldl}")

    def learn_from_offline_dataset(self, dataset: Dataset) -> None:
        # First, learn NSRTs.
        self._learn_nsrts(dataset.trajectories, online_learning_cycle=None)
        # Now, learn the LDL policy.
        self._learn_ldl()
        # Save the LDL policy.
        save_path = utils.get_approach_save_path_str()
        with open(f"{save_path}_None.ldl", "wb") as f:
            pkl.dump(self._current_ldl, f)

    def load(self, online_learning_cycle: Optional[int]) -> None:
        load_path = utils.get_approach_load_path_str()
        with open(f"{load_path}_{online_learning_cycle}.ldl", "rb") as f:
            self._current_ldl = pkl.load(f)

    def _create_search_operators(self) -> List[_PG3SearchOperator]:
        search_operator_classes = [
            _AddRulePG3SearchOperator,
            #TODO:  _AddConditionPG3SearchOperator,
        ]
        types = self._types
        preds = self._get_current_predicates()
        nsrts = self._get_current_nsrts()
        return [cls(types, preds, nsrts) for cls in search_operator_classes]

    def _create_heuristic(self) -> _PG3Heuristic:
        preds = self._get_current_predicates()
        abstract_train_tasks = [
            utils.create_abstract_task(task, preds)
            for task in self._train_tasks
        ]

        if CFG.pg3_heuristic == "policy_evaluation":
            return _PolicyEvaluationPG3Heuristic(abstract_train_tasks)

        raise NotImplementedError("Unrecognized pg3_heuristic: "
                                  f"{CFG.pg3_heuristic}.")


############################## Search Operators ###############################


class _PG3SearchOperator(abc.ABC):
    """Given an LDL policy, generate zero or more successor LDL policies."""

    def __init__(self, types: Set[Type], predicates: Set[Predicate],
                 nsrts: Set[NSRT]) -> None:
        self._types = types
        self._predicates = predicates
        self._nsrts = nsrts

    @abc.abstractmethod
    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        """Generate zero or more successor LDL policies."""
        raise NotImplementedError("Override me!")


class _AddRulePG3SearchOperator(_PG3SearchOperator):
    """An operator that adds new rules to an existing LDL policy."""

    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        import ipdb
        ipdb.set_trace()


################################ Heuristics ###################################


class _PG3Heuristic(abc.ABC):
    """Given an LDL policy, produce a score, with lower better."""

    def __init__(self, abstract_train_tasks: Sequence[AbstractTask]) -> None:
        self._abstract_train_tasks = abstract_train_tasks

    @abc.abstractmethod
    def __call__(self, ldl: LiftedDecisionList) -> float:
        """Produce a score, with lower better."""
        raise NotImplementedError("Override me!")


class _PolicyEvaluationPG3Heuristic(_PG3Heuristic):
    """Score a policy based on the number of train tasks it solves at the
    abstract level.

    The low-level space is ignored.
    """

    def __call__(self, ldl: LiftedDecisionList) -> float:
        unsolved_tasks = 0
        for abstract_task in self._abstract_train_tasks:
            if not self._ldl_solves_abstract_task(ldl, abstract_task):
                unsolved_tasks += 1
        return unsolved_tasks

    @staticmethod
    def _ldl_solves_abstract_task(ldl: LiftedDecisionList,
                                  abstract_task: AbstractTask) -> bool:
        atoms = abstract_task.init
        goal = abstract_task.goal
        for _ in range(CFG.horizon):
            if abstract_task.goal_holds(atoms):
                return True
            ground_nsrt = utils.query_ldl(ldl, atoms, goal)
            if ground_nsrt is None:
                return False
            atoms = utils.apply_operator(ground_nsrt, atoms)
        return abstract_task.goal_holds(atoms)
